{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this project, we will attempt to build a chacarter-level GPT language model which learnes to add two non-negative integers, i.e. given the input string \"a+b=c\", the model will be trained to predict the next character following a sliding context window.\n",
    "\n",
    "#### This is a simple next character prediction task. We will attempt two different versions of this task: 1) The integers of \"c\" are predicted left-to-right 2) the integers are predicted from right to left (i.e backward) which is typically how humans compute additions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['+', '0', '1', '10', '2', '3', '4', '5', '6', '7', '8', '9', '<*>', '<PAD>', '=']\n",
      "vocab_size = 15\n",
      "{'+': 0, '0': 1, '1': 2, '10': 3, '2': 4, '3': 5, '4': 6, '5': 7, '6': 8, '7': 9, '8': 10, '9': 11, '<*>': 12, '<PAD>': 13, '=': 14}\n"
     ]
    }
   ],
   "source": [
    "# first let's set up the token vocabulary for this problem\n",
    "# note that we have two special tokens '<*>' which denotes the beginning or end of a \n",
    "# sequence and the '<PAD>' token which is used for pre-padding sequences to ensure fixed length \n",
    "vocab = sorted(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '+', '=', '<END>', '<PAD>'])\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocabulary: {vocab}\")\n",
    "print(f\"vocab_size = {vocab_size}\")\n",
    "\n",
    "# tokenization\n",
    "ctoi = {vocab[i]:i for i in range(vocab_size)}\n",
    "itoc = {i:vocab[i] for i in range(vocab_size)}\n",
    "encode = lambda s: [ctoi[c] for c in s]  # converts a string to integer token sequence\n",
    "decode = lambda s: [itoc[ix] for ix in s]  # converts an integer token sequence to string of characters\n",
    "print(ctoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets implement the data loader which generates a batch of input-target pairs. We will make sure that the context block size will be large enough to see the entire problem string over multiple sliding windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1223)\n",
    "\n",
    "# generates input target pairs for a single problem string \"a+b=c\"\n",
    "def generate_batch(max_digits, block_size, batch_size, backward=False):\n",
    "\n",
    "    # make sure block size is big enough to hold the entire problem string\n",
    "    max_problem_size = 3*max_digits+2\n",
    "    assert block_size >= max_problem_size, f\"block_size needs to be at least {max_problem_size}\"\n",
    "\n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    for b in range(batch_size):\n",
    "\n",
    "        # randomly generate two integers\n",
    "        a = random.randint(0,10**max_digits-1)\n",
    "        b = random.randint(0,10**max_digits-1)\n",
    "        c = a + b\n",
    "\n",
    "        prompt = list(f\"{a}+{b}=\")\n",
    "        answer = list(f\"{c}\")\n",
    "        \n",
    "        if backward:\n",
    "            # reverse the digits of \"c\"\n",
    "            answer = reversed(answer)\n",
    "\n",
    "        #print(f\"prompt: {prompt}\")\n",
    "        #print(f\"answer: {answer}\")\n",
    "\n",
    "        # encolse with special token\n",
    "        prompt = ['<*>'] + prompt\n",
    "        answer = answer + ['<*>'] \n",
    "        problem = prompt+answer\n",
    "        tot_len = len(problem)\n",
    "\n",
    "        # post-pad the problem string to make it (block_size+1) long\n",
    "        problem = problem + ['<PAD>'] * (block_size+1-tot_len)\n",
    "        #print(f\"padded problem: {problem}\")\n",
    "\n",
    "        # tokenized input and target sequences        \n",
    "        input = torch.tensor(encode(problem[:block_size]))\n",
    "        target = torch.tensor(encode(problem[1:block_size+1]))\n",
    "        inputs.append(input)\n",
    "        targets.append(target)\n",
    "\n",
    "        #print(f\"context: {input} -- > target: {target}\")\n",
    "\n",
    "    # create input,target batch tensors\n",
    "    x = torch.stack(inputs).to(device)\n",
    "    y = torch.stack(targets).to(device)\n",
    "\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_digits = 5 # max number of digits for input integers 'a' and 'b'\n",
    "batch_size = 4\n",
    "block_size = 26 \n",
    "\n",
    "x, y = generate_batch(max_digits, block_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 26])\n",
      "tensor([[12, 11,  9,  7, 11,  2,  0, 10,  7,  4,  1, 10, 14,  2, 10,  4,  9, 11,\n",
      "         11, 12, 13, 13, 13, 13, 13, 13],\n",
      "        [12,  7,  9,  6,  7,  9,  0,  8,  9,  4,  6,  7, 14,  2,  4,  6,  9,  1,\n",
      "          4, 12, 13, 13, 13, 13, 13, 13],\n",
      "        [12,  9,  7,  2, 11, 11,  0,  8,  5,  6,  2,  9, 14,  2,  5, 10,  8,  2,\n",
      "          8, 12, 13, 13, 13, 13, 13, 13],\n",
      "        [12,  4,  7,  9, 11,  8,  0,  6,  2,  5,  1,  8, 14,  8,  9,  2,  1,  4,\n",
      "         12, 13, 13, 13, 13, 13, 13, 13]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
