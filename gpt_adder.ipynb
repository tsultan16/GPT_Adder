{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this project, we will attempt to build a chacarter-level GPT language model which learnes to add two non-negative integers, i.e. given the input string \"a+b=c\", the model will be trained to predict the next character following a sliding context window.\n",
    "\n",
    "#### This is a simple next character prediction task. We will attempt two different versions of this task: 1) The integers of \"c\" are predicted left-to-right 2) the integers are predicted from right to left (i.e backward) which is typically how humans compute additions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['+', '0', '1', '10', '2', '3', '4', '5', '6', '7', '8', '9', '<END>', '<PAD>', '<START>', '=']\n",
      "vocab_size = 16\n",
      "{'+': 0, '0': 1, '1': 2, '10': 3, '2': 4, '3': 5, '4': 6, '5': 7, '6': 8, '7': 9, '8': 10, '9': 11, '<END>': 12, '<PAD>': 13, '<START>': 14, '=': 15}\n"
     ]
    }
   ],
   "source": [
    "# first let's set up the token vocabulary for this problem\n",
    "# note that we have three special tokens: '<START>' which denotes the beginning of a problem\n",
    "# sequence, '<END>' denoting end of sequence and a '<PAD>' token which is used for post-padding sequences to ensure fixed length \n",
    "vocab = sorted(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '+', '=', '<START>', '<END>', '<PAD>'])\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocabulary: {vocab}\")\n",
    "print(f\"vocab_size = {vocab_size}\")\n",
    "\n",
    "# tokenization\n",
    "ctoi = {vocab[i]:i for i in range(vocab_size)}\n",
    "itoc = {i:vocab[i] for i in range(vocab_size)}\n",
    "encode = lambda s: [ctoi[c] for c in s]  # converts a string to integer token sequence\n",
    "decode = lambda s: [itoc[ix] for ix in s]  # converts an integer token sequence to string of characters\n",
    "print(ctoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets implement the data loader which generates a batch of input-target pairs. We will make sure that the context block size will be large enough to see the entire problem string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1223)\n",
    "\n",
    "# generates input target pairs for a single problem string \"a+b=c\"\n",
    "def generate_batch(block_size=26, batch_size=32, max_digits=5, backward=False):\n",
    "\n",
    "    # make sure block size is big enough to hold the entire problem string\n",
    "    max_problem_size = 3*max_digits+2\n",
    "    assert block_size >= max_problem_size, f\"block_size needs to be at least {max_problem_size}\"\n",
    "\n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    for b in range(batch_size):\n",
    "\n",
    "        # generate two random integers\n",
    "        a, b = np.random.randint(0,10**max_digits-1,2)\n",
    "        c = a + b\n",
    "\n",
    "        prompt = list(str(a).zfill(max_digits) + \"+\" + str(b).zfill(max_digits)+\"=\")\n",
    "        answer = list(str(c).zfill(max_digits+1))\n",
    "        \n",
    "        if backward:\n",
    "            # reverse the digits of \"c\"\n",
    "            answer = reversed(answer)\n",
    "\n",
    "        #print(f\"prompt: {prompt}\")\n",
    "        #print(f\"answer: {answer}\")\n",
    "\n",
    "        # encolse with special start and end tokens\n",
    "        prompt = ['<START>'] + prompt\n",
    "        answer = answer + ['<END>'] \n",
    "        problem = prompt+answer\n",
    "        tot_len = len(problem)\n",
    "\n",
    "        # post-pad the problem string to make it (block_size+1) long\n",
    "        problem = problem + ['<PAD>'] * (block_size+1-tot_len)\n",
    "        #print(f\"padded problem: {problem}\")\n",
    "\n",
    "        input = problem[:block_size]\n",
    "        target = problem[1:block_size+1]\n",
    "        #print(f\"context: {input} -- > target: {target}\")\n",
    "\n",
    "        # tokenized input and target sequences\n",
    "        input = torch.tensor(encode(input))\n",
    "        target = torch.tensor(encode(target))\n",
    "        #print(f\"Tokenized context: {input} -- > target: {target}\")\n",
    "\n",
    "        # mask out all character up to and including the '=' in the target (i.e. replace label with mask label: -1)\n",
    "        target[:2*max_digits+2] = -1\n",
    "        #print(f\"Masked target: {target}\")  \n",
    "\n",
    "        inputs.append(input)\n",
    "        targets.append(target)\n",
    "\n",
    "    # create input,target batch tensors\n",
    "    x = torch.stack(inputs).to(device)\n",
    "    y = torch.stack(targets).to(device)\n",
    "\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate an example batch\n",
    "max_digits = 5 # max number of digits for input integers 'a' and 'b'\n",
    "batch_size = 1\n",
    "block_size = 26 \n",
    "\n",
    "x, y = generate_batch(block_size, batch_size, max_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 26])\n",
      "tensor([[14,  5,  4,  5,  9,  7,  0, 10,  8,  7,  2,  8, 15,  2,  2, 10, 10, 11,\n",
      "          2, 12, 13, 13, 13, 13, 13, 13]], device='cuda:0')\n",
      "tensor([[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  2,  2, 10, 10, 11,  2,\n",
      "         12, 13, 13, 13, 13, 13, 13, 13]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, total_head_size, num_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        assert total_head_size % num_heads == 0, \"head_size needs to be integer multiple of num_heads\"\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.total_head_size = total_head_size \n",
    "        self.head_size = total_head_size // num_heads \n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # define parameters\n",
    "        self.key = nn.Linear(embedding_dim, self.total_head_size, bias=False)\n",
    "        self.query = nn.Linear(embedding_dim, self.total_head_size, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, self.total_head_size, bias=False)\n",
    "        self.attn_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # non-parameter tensor of lower triangular ones\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        # we also need to apply a linear projection to make the output residual the same dimension as the input\n",
    "        self.proj = nn.Linear(total_head_size, embedding_dim) \n",
    "        self.output_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "\n",
    "    # define forward pass, input shape: (B,T,C) where B=batch size, T=block_size, C=embedding_dim\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B,T,H) where H is the total_head_size\n",
    "        q = self.query(x) # (B,T,H)\n",
    "        v = self.value(x) # (B,T,H)\n",
    "\n",
    "        # reshape (B,T,H) --> (B,T,n,h), where n=num_heads and h=head_size and H=n*h\n",
    "        k = k.view(B,T,self.num_heads,self.head_size) \n",
    "        q = q.view(B,T,self.num_heads,self.head_size) \n",
    "        v = v.view(B,T,self.num_heads,self.head_size) \n",
    "\n",
    "        # now we transpose so that the num_heads is the second dimension followed by T,h\n",
    "        # this allows us to batch matrix mutliply for all heads simulataneously to compute their attention weights\n",
    "        # (B,T,n,h) --> (B,n,T,h) \n",
    "        k = k.transpose(1,2) \n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "\n",
    "        # use pytorch built-in function for faster computation of attention scores (set the 'is_causal' parameter for applying causal masking)\n",
    "        out = F.scaled_dot_product_attention(q,k,v,dropout_p=self.dropout_rate if self.training else 0,is_causal=True)\n",
    "\n",
    "        # we can transpose the output from (B,n,T,h) --> (B,T,n,h)\n",
    "        # since the last two dimensions of the transposed tensor are non-contiguous, we apply \n",
    "        # contiguous() which return a contiguous tensor\n",
    "        out = out.transpose(1,2).contiguous()\n",
    "\n",
    "        # finally we collapse the last two dimensions to get the concatenated output, (B,T,n,h) --> (B,T,n*h) \n",
    "        out = out.view(B,T,self.total_head_size)\n",
    "\n",
    "        # now we project the concatenated output so that it has the same dimensions as the multihead attention layer input\n",
    "        # (we need to add it with the input because of the residual connection, so need to be same size) \n",
    "        out = self.proj(out) # (B,T,C) \n",
    "\n",
    "        # apply dropout\n",
    "        out = self.output_dropout(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "# a simple mlp \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        # we add extra computations by growing out the feed-forward hidden size by a factor of 4\n",
    "        # we also add an extra linear layer at the end to project the residual back to same dimensions as input\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 4*embedding_dim),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*embedding_dim, embedding_dim), \n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "    \n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "# transformer block with residual connection and layer norm\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, head_size, num_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(block_size, embedding_dim, head_size, num_heads, dropout_rate) # multi-head attention layer \n",
    "        self.ff = FeedForward(embedding_dim, dropout_rate)   # feed-forward layer\n",
    "        self.ln1 = nn.LayerNorm(embedding_dim) # layer norm at input of multi-head attention\n",
    "        self.ln2 = nn.LayerNorm(embedding_dim) # layer norm at input of feed-forward\n",
    "\n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        # residual connection between input and multi-head attention output\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        # residual connection between multi-head attention output and feed-forward output\n",
    "        x = x + self.ff(self.ln2(x)) \n",
    "        return x\n",
    "    \n",
    "\n",
    "# language model with multiple transformer blocks\n",
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, embedding_dim, head_size, num_heads, num_blocks, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = head_size\n",
    "        self.hum_heads = num_heads\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        '''\n",
    "        Define model parameters\n",
    "        '''\n",
    "        # token embedding layer \n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim) # shape: (vocab_size,C)\n",
    "        # position embedding layer\n",
    "        self.pos_embedding = nn.Embedding(block_size, embedding_dim) # shape: (T,C)\n",
    "\n",
    "        # stack of transformer blocks\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(block_size, embedding_dim, head_size, num_heads, dropout_rate) for _ in range(num_blocks)])\n",
    "\n",
    "        # we also add a layer norm before the final output layer\n",
    "        self.ln_f = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        # output layer logits\n",
    "        self.lm_head = nn.Linear(head_size, vocab_size) # shape: (h,vocab_size)\n",
    "\n",
    "\n",
    "        # forward pass takes in a batch of input token sequences of shape (B,T) and corresponding targets of shape (B,T)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T =idx.shape\n",
    "        # get token embeddings\n",
    "        token_embeds = self.token_embedding(idx) # (B,T,C)\n",
    "        # add positional encoding\n",
    "        pos_embeds = self.pos_embedding(torch.arange(T, device=device)) # (T,C) \n",
    "        x = token_embeds + pos_embeds # (B,T,C)\n",
    "        # pass through transformer blocks\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        # apply layer norm\n",
    "        x = self.ln_f(x)  # (B,T,C)\n",
    "        # compute output logits \n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B,T,vocab_size = logits.shape\n",
    "            # reshape the logits and targets such that batch of input sequences are flattened into a single big input sequence\n",
    "            # i.e. (B,T) --> (B*T)\n",
    "            logits = logits.view(B*T,vocab_size) # reshaped to (B*T,vocab_size)\n",
    "            targets = targets.view(B*T) # reshaped to (B*T)\n",
    "            # compute cross entropy loss (i.e. average negative log likelihood)\n",
    "            # ignore the masked target labels (i.e. the labels which are -1)\n",
    "            loss = F.cross_entropy(logits, targets, ignore_index=-1)\n",
    "        return logits, loss\n",
    "    \n",
    "\n",
    "    # generates new sequences continuing from a given batch of context tokens\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        self.eval() # swicth to inference mode\n",
    "        # batch of contexts, idx has shape (B,T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # since we're using positional encoding, we need to crop idx if input sequence length exceeds block size (keep last block_size tokens)\n",
    "            idx_crop = idx[:,-self.block_size:] \n",
    "            # get predictions\n",
    "            logits, _ = self(idx_crop) # shape: (B,T,C)\n",
    "            # for each context sequence (in the batch), compute the probability of the next token using the logits of the last token in the context sequence\n",
    "            logits = logits[:,-1,:] # shape: (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            # sample from the probability distribution to get next token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # shape: (B,1)\n",
    "            # append to the current context\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # shape: (B,T+1)\n",
    "\n",
    "        self.train() # swicth to train mode\n",
    "\n",
    "        return idx\n",
    "    \n",
    "\n",
    "# evaluating training and validation losses averaged over lots of batches\n",
    "@torch.no_grad() # disable gradient tracking\n",
    "def estimate_loss(model, eval_iters, block_size, batch_size):\n",
    "    out = {}\n",
    "    model.eval() # swicth to inference mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = generate_batch(block_size=block_size,batch_size=batch_size) \n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean() \n",
    "    model.train() # switch back to training mode\n",
    "    return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gpt_adder(a, b, max_digits):\n",
    "    # generate a single sequences using the model with start token \n",
    "    input = ['<START>'] + list(str(a).zfill(max_digits)+\"+\"+str(b).zfill(max_digits)+\"=\")\n",
    "    input = [encode(input)]\n",
    "    idx = torch.tensor(input,dtype=torch.long, device=device)\n",
    "    #print(idx)\n",
    "    generated_seq = m.generate(idx, max_new_tokens=30)[0].tolist()\n",
    "    # Decode integer tokens into characters\n",
    "    generated_seq = decode(generated_seq)\n",
    "    # remove pad tokens\n",
    "    generated_seq = list(filter(lambda c: c!='<PAD>', generated_seq))\n",
    "    print(\"\\nGenerated sequence:\\n\",\"\".join(generated_seq)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 0.60136 M\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "block_size = 26\n",
    "embedding_dim = 128\n",
    "head_size = embedding_dim\n",
    "num_heads = 4\n",
    "num_blocks = 3\n",
    "dropout_rate = 0.2\n",
    "max_iters = 20000\n",
    "learning_rate = 5e-4\n",
    "eval_interval = 200\n",
    "eval_iters = 100\n",
    "\n",
    "model = TransformerLanguageModel(vocab_size=vocab_size, block_size=block_size, embedding_dim=embedding_dim, head_size=head_size, num_heads=num_heads, num_blocks=num_blocks, dropout_rate=dropout_rate)\n",
    "# move model to device\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "\n",
    "num_params = sum(p.numel() for p in m.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20000, Train Loss: 0.000, Val Loss: 0.001: 100%|██████████| 20000/20000 [08:29<00:00, 39.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "train_loss = None\n",
    "val_loss = None\n",
    "\n",
    "pbar = tqdm(range(max_iters), desc=\"Epochs\")\n",
    "for epoch in pbar:\n",
    "    # sample a batch of trainin data\n",
    "    xb, yb = generate_batch(block_size=block_size, batch_size=batch_size)\n",
    "    # evaluate the loss\n",
    "    _, loss = m(xb, yb)\n",
    "    # reset parameter gradients\n",
    "    optimizer.zero_grad(set_to_none=True) \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % eval_interval == 0:\n",
    "        losses = estimate_loss(m, eval_iters,block_size, batch_size)\n",
    "        #print(f\"epoch: {epoch}, training loss: {losses['train'].item()}, validation loss: {losses['val'].item()}\")     \n",
    "        train_loss = losses['train'].item()\n",
    "        val_loss = losses['val'].item()\n",
    "\n",
    "    pbar.set_description(f\"Epoch {epoch + 1}, Train Loss: {train_loss:.3f}, Val Loss: {val_loss:.3f}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sequence:\n",
      " <START>82292+01013=083305<END>\n"
     ]
    }
   ],
   "source": [
    "test_gpt_adder(82292, 1013, max_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sequence:\n",
      " <START>09912+00543=010455<END>\n"
     ]
    }
   ],
   "source": [
    "test_gpt_adder(9912, 543, max_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
