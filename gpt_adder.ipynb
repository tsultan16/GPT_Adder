{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this project, we will attempt to build a minimal GPT model which learnes to add two integers, i.e. given the input string \"a+b=\", the model will be trained to predict the integer sequence \"c\", where c=a+b.\n",
    "\n",
    "#### This is a simple next character prediction task. We will attempt two different versions of this task: 1) The integers of \"c\" are predicted left-to-right 2) the integers are predicted from right to left (i.e backward) which is typically how humans compute additions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['+', '0', '1', '10', '2', '3', '4', '5', '6', '7', '8', '9', '<*>', '<PAD>', '=']\n"
     ]
    }
   ],
   "source": [
    "# first let's set up the token vocabulary for this problem\n",
    "# note that we have two special tokens '<*>' which denotes the beginning or end of a \n",
    "# sequence and the '<PAD>' token which is used for pre-padding sequences to ensure fixed length \n",
    "vocab = sorted(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '+', '=', '<*>', '<PAD>'])\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocabulary: {vocab}\")\n",
    "\n",
    "# tokenization\n",
    "ctoi = {vocab[i]:i for i in range(vocab_size)}\n",
    "itoc = {i:vocab[i] for i in range(vocab_size)}\n",
    "encode = lambda s: [ctoi[c] for c in s]  # converts a string to integer token sequence\n",
    "decode = lambda s: [itoc[ix] for ix in s]  # converts an integer token sequence to string of characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, lets implement the data loader which generates input-target pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
